{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実行フロー\n",
    "1. ページングページURLリストを作成。\n",
    "2. 記事URLリストを取得・保存。(記事内ページングを考慮)\n",
    "3. 記事URLリストからHTMLをそのまま取得。(URLと一緒に)\n",
    "4. 取得したHTMLから本文・タイトル・日付を抽出。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ページングあり]\n",
    "http://number.bunshun.jp/category/column?page=900\n",
    "https://victorysportsnews.com/?p=630 \n",
    "https://sports.yahoo.co.jp/column/list?type=sportsnavi&p=1250\n",
    "https://azrena.com/post/category/article/all/page/50/\n",
    "https://www.shimotsuke.co.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page=90\n",
    "https://www.chibanippo.co.jp/newspack/sports?page=2763\n",
    "http://www.kanaloco.jp/sports/generalsports/298/\n",
    "http://www.isenp.co.jp/category/sports/page/56/\n",
    "http://www.topics.or.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page=180\n",
    "https://www.nishinippon.co.jp/nsp/archive/?page=1063\n",
    "http://www.saga-s.co.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page=610\n",
    "\n",
    "[特定のスポーツだけのサイト]\n",
    "http://www.hb-nippon.com/column\n",
    "https://rugby-rp.com/news_list.asp\n",
    "https://www.thetennisdaily.jp/\n",
    "https://full-count.jp/\n",
    "https://web.gekisaka.jp/\n",
    "https://timely-web.jp/\n",
    "http://basket-count.com/\n",
    "\n",
    "[最近の記事のみ or ページングなし]\n",
    "https://www.jsports.co.jp/press/new/index4.html\n",
    "http://www.nhk.or.jp/sports-story/list_article.html\n",
    "https://mainichi.jp/sports/20\n",
    "https://www.nikkansports.com/sports/news/\n",
    "http://www.minpo.jp/sports/\n",
    "https://www.jomo-news.co.jp/sports/domestic?s=30&p=142\n",
    "http://www.tokyo-np.co.jp/article/sports/\n",
    "http://www.niigata-nippo.co.jp/news/sports/\n",
    "http://webun.jp/card/12028\n",
    "http://www.hokkoku.co.jp/index.php?genre=sports\n",
    "http://www.shinmai.co.jp/news/nagano/web_sports_list.html\n",
    "https://www.gifu-np.co.jp/news/sports/index_4.html\n",
    "http://www.at-s.com/sports/national/\n",
    "http://a-chikara.com/\n",
    "https://www.nikkei.com/sports/column/ \n",
    "https://news.goo.ne.jp/topstories/backnumber/sports/ \n",
    "https://www.jiji.com/jc/list?g=spo&d=date4\n",
    "http://www.asahi.com/sports/general/\n",
    "https://www.hochi.co.jp/sports/\n",
    "http://www.sanspo.com/\n",
    "https://www.kahoku.co.jp/sports/\n",
    "http://yamagata-np.jp/news/gunretop_6.php\n",
    "http://www.chunichi.co.jp/s/article/sports.html\n",
    "http://www.kyoto-np.co.jp/sports/article/newslist\n",
    "https://www.kobe-np.co.jp/news/sports/\n",
    "http://www.agara.co.jp/news/sports/\n",
    "http://www.nnn.co.jp/knews/sports_worldgames.html\n",
    "http://www.sanin-chuo.co.jp/www/genre/1000100000037/index.html\n",
    "http://www.shikoku-np.co.jp/sports/local_pro/archive-201807\n",
    "https://www.ehime-np.co.jp/online/sports/global_sports/list/\n",
    "https://kumanichi.com/news/zenkoku/sports/\n",
    "https://373news.com/_news/topic.php?topicid=12&storyid=94015\n",
    "\n",
    "\n",
    "[会員限定記事が多い]\n",
    "https://www.hokkaido-np.co.jp/sports/s_general/163/\n",
    "https://ibarakinews.jp/hp/list.php?elem=sports&pg=3\n",
    "http://www.sanyonews.jp/okayama_sports/526\n",
    "http://www.okinawatimes.co.jp/subcategory/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page=300\n",
    "https://www.toonippo.co.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page=260\n",
    "https://www.sakigake.jp/news/list/scd/100001006/63/?nv=sports\n",
    "\n",
    "\n",
    "[ページング困難]\n",
    "https://cyclestyle.net/\n",
    "https://thepage.jp/category/sports/\n",
    "https://www.daily.co.jp/general/?pg=list\n",
    "https://www.iwate-np.co.jp/article/category/sports\n",
    "http://sports.sannichi.co.jp/smp/sports/news/index.html\n",
    "https://www.nagasaki-np.co.jp/news/sports/\n",
    "http://www.oita-press.co.jp/1004000000\n",
    "https://sportiva.shueisha.co.jp/clm/index_790.php\n",
    "https://ryukyushimpo.jp/news/page/850/?genre=sports\n",
    "\n",
    "\n",
    "[ページ数が少ない]\n",
    "https://www.nara-np.co.jp/news/sports.html?p=19\n",
    "https://www.47news.jp/sports/sports-column?page=20\n",
    "http://www.chugoku-np.co.jp/local/news/list.php?category_id=255&page=18\n",
    "http://www.minato-yamaguchi.co.jp/yama/wsports.html\n",
    "http://www.the-miyanichi.co.jp/sports/category_21/page_40.html\n",
    "https://perfectanavi.com/column/page/14/\n",
    "http://www.j-n.co.jp/psm/page/6/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "http://number.bunshun.jp/category/column?page=900\n",
    "https://victorysportsnews.com/?p=630\n",
    "http://www.saga-s.co.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page=610\n",
    "https://sports.yahoo.co.jp/column/list?type=sportsnavi&p=1250\n",
    "https://azrena.com/post/category/article/all/page/50/\n",
    "https://www.shimotsuke.co.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page=90\n",
    "https://www.chibanippo.co.jp/newspack/sports?page=2763\n",
    "http://www.kanaloco.jp/sports/generalsports/298/\n",
    "http://www.isenp.co.jp/category/sports/page/56/\n",
    "http://www.topics.or.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page=180\n",
    "https://www.nishinippon.co.jp/nsp/archive/?page=1063\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class UrlGetter:\n",
    "    def __init__(self):\n",
    "        self.max_pages = [900, 630, 610, 1250, 50, 90, 2763, 298, 56, 180, 1063]\n",
    "    \n",
    "    def get_0(self, page):\n",
    "        return \"http://number.bunshun.jp/category/column?page={}\".format(page)\n",
    "    \n",
    "    def get_1(self, page):\n",
    "        return \"https://victorysportsnews.com/?p={}\".format(page)\n",
    "    \n",
    "    def get_2(self, page):\n",
    "        return \"http://www.saga-s.co.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page={}\".format(page)\n",
    "\n",
    "    def get_3(self, page):\n",
    "        return \"https://sports.yahoo.co.jp/column/list?type=sportsnavi&p={}\".format(page)\n",
    "    \n",
    "    def get_4(self, page):\n",
    "        return \"https://azrena.com/post/category/article/all/page/{}/\".format(page)\n",
    "    \n",
    "    def get_5(self, page):\n",
    "        return \"https://www.shimotsuke.co.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page={}\".format(page)\n",
    "    \n",
    "    def get_6(self, page):\n",
    "        return \"https://www.chibanippo.co.jp/newspack/sports?page={}\".format(page)\n",
    "    \n",
    "    def get_7(self, page):\n",
    "        return \"http://www.kanaloco.jp/sports/generalsports/{}/\".format(page)\n",
    "    \n",
    "    def get_8(self, page):\n",
    "        return \"http://www.isenp.co.jp/category/sports/page/{}/\".format(page)\n",
    "    \n",
    "    def get_9(self, page):\n",
    "        return \"https://www.nishinippon.co.jp/nsp/archive/?page={}\".format(page)\n",
    "    \n",
    "    def get_10(self, page):\n",
    "        return \"http://www.topics.or.jp/category/news-original/%E3%82%B9%E3%83%9D%E3%83%BC%E3%83%84?page={}\".format(page)\n",
    "        \n",
    "    def get_url(self, site_num, *params):\n",
    "        return getattr(self, \"get_{}\".format(site_num))(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleGetter:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        r = requests.get(base_url)\n",
    "        self.soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    def get_0(self):\n",
    "        section = self.soup.find(\"section\", {\"class\": \"category-archive\"})\n",
    "        return list(set([\"http://number.bunshun.jp\"+result['href'] for result in section.find_all(\"a\") if result['href'].startswith(\"/articles/\")]))\n",
    "\n",
    "    def get_1(self):\n",
    "        section = self.soup.find(\"div\", {\"class\":\"article-list-page\"})\n",
    "        return list(set([\"https://victorysportsnews.com\"+result['href']+\"/original\" for result in section.find_all(\"a\", {\"class\":\"link-card-inner\"})]))\n",
    "    \n",
    "    def get_2(self):\n",
    "        section = self.soup.find(\"div\", {\"id\": \"article-category-list\"})\n",
    "        return list(set(\"http://www.saga-s.co.jp\" + result['href'] for result in section.find_all(\"a\") if result['href'].startswith(\"/articles/\")))\n",
    "    \n",
    "    def get_3(self):\n",
    "        section = self.soup.find_all(\"p\", {\"class\":\"articleTitle\"})\n",
    "        return list(set(result.find(\"a\")['href'] for result in section))\n",
    "    \n",
    "    def get_4(self):\n",
    "        return list(set(result['href'] for result in self.soup.find_all(\"a\", {\"class\": \"panel\"})))\n",
    "    \n",
    "    def get_5(self):\n",
    "        section = self.soup.find(\"ul\", {\"class\": \"article-category-list\"})\n",
    "        return list(set(\"https://www.shimotsuke.co.jp\" + result['href'] for result in section.find_all(\"a\") if result['href'].startswith(\"/articles/\")))\n",
    "    \n",
    "    def get_6(self):\n",
    "        section = self.soup.find(\"div\", {\"class\": \"item-list\"})\n",
    "        return list(set(\"https://www.chibanippo.co.jp\" + result['href'] for result in section.find_all(\"a\")))\n",
    "    \n",
    "    def get_7(self):\n",
    "        section = self.soup.find(\"ul\", {\"class\": \"post-grid\"})\n",
    "        return list(set(\"http://www.kanaloco.jp\" + result['href'] for result in section.find_all(\"a\")))\n",
    "    \n",
    "    def get_8(self):\n",
    "        section = self.soup.find_all(\"h3\", {\"class\": \"entry-title\"})\n",
    "        return list(set(result.find(\"a\")['href'] for result in section))\n",
    "        \n",
    "    def get_9(self):\n",
    "        section = self.soup.find(\"ul\", {\"class\": \"NP-articleList\"})\n",
    "        return list(set(\"https://www.nishinippon.co.jp\"+result['href'] for result in section.find_all(\"a\")))\n",
    "    \n",
    "    def get_10(self):\n",
    "        section = self.soup.find(\"div\", {\"id\": \"article-category-list\"})\n",
    "        return list(set(\"http://www.topics.or.jp\" + result['href'] for result in section.find_all(\"a\") if result['href'].startswith(\"/articles/\")))\n",
    "    \n",
    "    def get_articles(self, site_num):\n",
    "        return getattr(self, \"get_{}\".format(site_num))()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sports.yahoo.co.jp/column/detail/201807220002-spnavi',\n",
       " 'https://2020.yahoo.co.jp/column/detail/201807170006-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807220001-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807200005-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807210007-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807230003-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807200008-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807210003-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807210001-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807230002-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807200002-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807210004-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807210002-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807210005-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807020002-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807220004-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807220003-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807210006-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807230001-spnavi',\n",
       " 'https://sports.yahoo.co.jp/column/detail/201807220005-spnavi']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_num = 3\n",
    "urlgetter = UrlGetter()\n",
    "base_url = urlgetter.get_url(site_num, 1)\n",
    "ArticleGetter(base_url).get_articles(site_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class ArticleExtractor:\n",
    "    def __init__(self, target_url):\n",
    "        self.target_url = target_url\n",
    "        self.driver = webdriver.PhantomJS()\n",
    "        self.driver.get(target_url)\n",
    "        self.soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        [s.extract() for s in self.soup('script')]\n",
    "        [s.extract() for s in self.soup('link')]\n",
    "    \n",
    "    def extract_0(self):\n",
    "        tmp = self.soup.find(\"div\", {\"class\": \"list-pagination-02\"})\n",
    "        inner_pages = [\"http://number.bunshun.jp\" + d['href'] for d in tmp.find_all(\"a\")]\n",
    "        title = self.soup.find(\"h2\").get_text()\n",
    "        day = self.soup.find(\"p\", {\"class\": \"posted\"})\n",
    "        day.find(\"span\").extract()\n",
    "        day = day.get_text()\n",
    "        body = self.soup.find(\"div\", {\"class\": \"single\"}).get_text()\n",
    "        for page in inner_pages:\n",
    "            tmp_soup = BeautifulSoup(requests.get(page).content, \"html.parser\")\n",
    "            [s.extract() for s in tmp_soup('script')]\n",
    "            [s.extract() for s in tmp_soup('link')]\n",
    "            body += tmp_soup.find(\"div\", {\"class\": \"single\"}).get_text()\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_1(self):\n",
    "        headline = self.soup.find(\"div\", {\"class\": \"article-headline\"})\n",
    "        title = headline.find(\"h1\").get_text()\n",
    "        day = headline.find(\"time\").get_text()\n",
    "        body = headline.find(\"p\").get_text()\n",
    "        abody = self.soup.find(\"div\", {\"class\": \"article-body\"})\n",
    "        abody.find('div', {\"class\": \"author-status\"}).extract()\n",
    "        [s.extract() for s in abody.find_all(\"a\")]\n",
    "        body += ''.join([d.get_text() for d in abody.find_all(\"p\")])\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_2(self):\n",
    "        title = self.soup.find(\"h1\").get_text()\n",
    "        day = self.soup.find(\"div\", {\"class\": \"pubdate\"}).get_text()\n",
    "        abody = self.soup.find(\"div\", {\"class\": \"article-body\"})\n",
    "        body = ''.join(d.get_text() for d in abody.find_all(\"p\"))\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_3(self):\n",
    "        title = self.soup.find(\"div\", {\"class\":\"columnTitle\"}).get_text()\n",
    "        day = self.soup.find(\"li\", {\"class\":\"postDate\"}).get_text()\n",
    "        body = self.soup.find(\"div\", {\"class\":\"mainSection\"}).get_text()\n",
    "        parsed_uri = urlparse(self.target_url)\n",
    "        top_url = '{uri.scheme}://{uri.netloc}'.format(uri=parsed_uri)\n",
    "        inner_pages = list(set([top_url + d['href'] for d in self.soup.find(\"div\", {\"class\":\"navPage\"}).find_all(\"a\")]))\n",
    "        for page in inner_pages:\n",
    "            self.driver.get(page)\n",
    "            tmp_soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "            [s.extract() for s in tmp_soup('script')]\n",
    "            [s.extract() for s in tmp_soup('link')]\n",
    "            body += tmp_soup.find(\"div\", {\"class\": \"mainSection\"}).get_text()\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_4(self):\n",
    "        title = self.soup.find(\"h1\", {\"class\": \"title\"}).get_text()\n",
    "        day = self.soup.find(\"span\", {\"class\": \"date\"}).get_text()\n",
    "        content = self.soup.find(\"div\", {\"class\": \"m-article-content\"})\n",
    "        body = ''.join([p.get_text() for p in content.find_all(\"p\")])\n",
    "        body = re.sub('<[^<]+?>', '', body)\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_5(self):\n",
    "        title = self.soup.find(\"h1\", {\"class\": \"title\"}).get_text()\n",
    "        day = self.soup.find(\"div\", {\"class\": \"pubdate\"}).get_text()\n",
    "        content = self.soup.find(\"article\")\n",
    "        content.find(\"div\", {\"class\": \"smart-link\"}).extract()\n",
    "        body = ''.join([d.get_text() for d in content.find_all(\"p\")])\n",
    "        body = re.sub('<[^<]+?>', '', body)\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_6(self):\n",
    "        title = self.soup.find(\"h1\", {\"class\": \"title\"}).get_text()\n",
    "        day = self.soup.find(\"div\", {\"class\": \"submitted\"}).get_text()\n",
    "        content = self.soup.find(\"article\")\n",
    "        body = ''.join([d.get_text() for d in content.find_all(\"p\")])\n",
    "        body = re.sub('<[^<]+?>', '', body)\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_7(self):\n",
    "        title = self.soup.find(\"h1\", {\"id\": \"post-title\"}).get_text()\n",
    "        day = self.soup.find(\"li\", {\"class\": \"date\"}).get_text()\n",
    "        body = self.soup.find(\"div\", {\"id\": \"post-content\"}).get_text()\n",
    "        body = re.sub('<[^<]+?>', '', body)\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_8(self):\n",
    "        title = self.soup.find(\"h1\", {\"class\": \"entry-title\"}).get_text()\n",
    "        day = self.soup.find(\"span\", {\"class\": \"entry-meta-date\"}).get_text()\n",
    "        content = self.soup.find(\"div\", {\"class\": \"entry-content\"})\n",
    "        body = ''.join([d.get_text() for d in content.find_all(\"p\")])\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_9(self):\n",
    "        title = self.soup.find(\"h1\").get_text()\n",
    "        day = self.soup.find(\"time\").get_text()\n",
    "        body = self.soup.find(\"div\", {\"class\": \"NP-articleText\"}).get_text()\n",
    "        body = re.sub('<[^<]+?>', '', body)\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_10(self):\n",
    "        title = self.soup.find(\"h1\", {\"class\": \"title\"}).get_text()\n",
    "        day = self.soup.find(\"div\", {\"class\": \"pubdate\"}).get_text()\n",
    "        body = self.soup.find(\"div\", {\"class\": \"article-body\"}).get_text()\n",
    "        body = re.sub('<[^<]+?>', '', body)\n",
    "        return title, body, day\n",
    "    \n",
    "    def extract_article(self, site_num):\n",
    "        title, body, day = getattr(self, \"extract_{}\".format(site_num))()\n",
    "        title = title.replace(\"\\n\", \" \").replace(\"\\u3000\", \" \").replace(\"\\r\",\"\").strip()\n",
    "        body = body.replace(\"\\n\", \" \").replace(\"\\u3000\", \" \").replace(\"\\r\", \"\").strip()\n",
    "        body = body.replace(\"\\xa0\", \"\")\n",
    "        day = day.replace(\"\\n\", \" \").replace(\"\\u3000\", \" \").replace(\"\\r\", \"\").strip()\n",
    "        return title, body, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/selenium/webdriver/phantomjs/webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead\n",
      "  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('徳島ヴォルティス、大宮に２－１で競り勝つ 前川先制、岩尾が決勝弾',\n",
       " '拡大する   前半、シュートを放つ徳島のＦＷウタカ（中央）＝鳴門ポカリスエットスタジアム                サッカーＪ２徳島ヴォルティスは２１日、ホーム・鳴門ポカリスエットスタジアムで大宮アルディージャと対戦し、後半２ゴールを挙げて２ー１で競り勝った。リーグ戦で９試合負けなしだった大宮に黒星をつけた。徳島は９勝４分け１１敗で、勝ち点を３１とした。順位は１５位。  徳島は新加入のＦＷピーター・ウタカが先発した。前半、序盤は徳島がボールを保持してチャンスをつくったが、中盤になると、大宮にボールを持たれ、セットプレーからゴールに迫られるなど、１５分ごとに攻撃の主導権が入れ替わる展開。２９分には、大宮に中盤でパスカットされてカウンターを受けたが、シュートはポストに当たり失点を免れた。  その後、徳島が反撃。３０分、島屋からのボールをウタカが収め、パスを受けた前川大河がゴール左のニアサイドを狙ってシュート。しかし、ＧＫの好守でネットは揺らせなかった。前川は３６分にもミドルで狙ったが枠を捉えられなかった。終盤にはウタカがペナルティエリア内で切り返してシュートを放つも阻まれ、直後のコーナーキックのヘディングシュートも枠をそれ、０－０で前半を終えた。  後半はめまぐるしく攻守が入れ替わる展開だったが、１１分、左サイドの岩尾憲が送ったパスに前川大河が抜けだし、左足で逆サイドのネットにボールを流し込んで先制した。直後に、大宮のＧＫから始まったカウンターを防ぎきれずに同点とされたが、２１分にウタカからのパスを受けた岩尾がシュートを決めて勝ち越した。その後、左サイドの大本祐槻の積極的な攻め上がりや前線のウタカがチャンスをつくるなど攻勢を保った。  大宮が前線に長身の選手を投入するなど攻撃の圧力を強め、押し込まれる時間帯もあったが、守備陣が粘り強くはね返し、１点のリードを守り切った。 ◇明治安田Ｊ２第２４節  鳴門ポカリスエットスタジアム  観衆５８０１人 徳島 ２－１ 大宮    ０－０    ２－１ 【得点者】徳島＝前川大河（後半１１分）岩尾憲（後半２１分）      大宮＝大前元紀（後半１４分）',\n",
       " '7/21  21:30')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_type = 10\n",
    "urlgetter = UrlGetter()\n",
    "base_url = urlgetter.get_url(site_type, 1)\n",
    "target_url = ArticleGetter(base_url).get_articles(site_type)[3]\n",
    "ArticleExtractor(target_url).extract_article(site_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sitenum(url):\n",
    "    if \"number.bunshun.jp\" in url:\n",
    "        return 0\n",
    "    elif \"victorysportsnews.com\" in url:\n",
    "        return 1\n",
    "    elif \"www.saga-s.co.jp\" in url:\n",
    "        return 2\n",
    "    elif \"azrena.com\" in url:\n",
    "        return 4\n",
    "    elif \"www.shimotsuke.co.jp\" in url:\n",
    "        return 5\n",
    "    elif \"www.chibanippo.co.jp\" in url:\n",
    "        return 6\n",
    "    elif \"www.kanaloco.jp\" in url:\n",
    "        return 7\n",
    "    elif \"www.isenp.co.jp\" in url:\n",
    "        return 8\n",
    "    elif \"www.nishinippon.co.jp\" in url:\n",
    "        return 9\n",
    "    elif \"www.topics.or.jp\" in url:\n",
    "        return 10\n",
    "    elif \"yahoo.co.jp\" in url:\n",
    "        return 3\n",
    "    else:\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_urllist(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        urllist = f.readlines()\n",
    "    return urllist\n",
    "\n",
    "urllist = read_urllist(\"urllist.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156975"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "with open(\"extracted_data.csv\", \"a\") as f:\n",
    "    f.write(\"url,title,body,day\\n\")\n",
    "    for url in tqdm(urllist):\n",
    "        try:\n",
    "            title, body, day = ArticleExtractor(url).extract_article(detect_sitenum(url))\n",
    "            f.write(\"{},{},{},{}\".format(url,title,body,day))\n",
    "        except:\n",
    "            print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
